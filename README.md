# ðŸ”¥ RNN-LSTM-Attention: Sequence Modeling in PyTorch

This repository is part of my **PyTorch Research Mastery Series** â€” a self-driven exercise to implement, document, and modularize core sequence-model architectures from scratch using **pure PyTorch**.

It includes custom implementations of:
- A fully parameterized **RNNCell** (without relying on `nn.RNN`)
- A modular **CustomLSTM** with bidirectional support
- An additive **Attention Mechanism** (Bahdanau-style)
- A composite **LSTM + Attention** model designed for flexible sequence tasks  

All models are written **from first principles**, emphasizing **clarity, reproducibility, and research-grade structure**.

---

## ðŸ§  Overview

The goal of this project was to understand and re-implement the fundamental building blocks of sequence modeling architectures used in modern AI research.  
Each component is structured in an isolated module (`models/`), with clean data abstraction, configuration, and training logic â€” mirroring FAIR and DeepMind-style internal repos.

The full pipeline supports **sequence regression/classification** tasks and can handle any CSV-based or tensor-based sequential dataset.

---

## ðŸ§© Architecture Components

| Component | File | Description |
|------------|------|-------------|
| ðŸ§± `RNNCell` | `models/rnn_cell.py` | Implements a basic recurrent cell using explicit weight matrices and tanh activation |
| âš™ï¸ `CustomLSTM` | `models/custom_lstm.py` | Modular wrapper around PyTorch's `nn.LSTM` with support for bidirectional passes |
| ðŸŽ¯ `CustomAttention` | `models/attention.py` | Implements additive attention with trainable parameters (W_a, U_a, v_a) |
| ðŸ§© `LSTMAttention` | `models/lstm_attention.py` | Combines LSTM encoder with attention for context-aware output |
| ðŸ“¦ `data_loader.py` |  | Dataset + DataLoader utilities for real sequential datasets (e.g., time series, signals, etc.) |
| âš™ï¸ `config.py` |  | Global hyperparameter configuration (hidden size, layers, LR, etc.) |
| ðŸ§® `utils/metrics.py` |  | Evaluation metrics (MSE, RMSE, RÂ²) for regression-type outputs |

---

## ðŸ§­ Folder Structure

```
rnn_lstm_attention/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ config.py
â”œâ”€â”€ data_loader.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ rnn_cell.py
â”‚   â”œâ”€â”€ custom_lstm.py
â”‚   â”œâ”€â”€ attention.py
â”‚   â”œâ”€â”€ lstm_attention.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ metrics.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ your_dataset.csv
â””â”€â”€ README.md
```

---

## ðŸ§ª Training

Run the main script with any real dataset (CSV or tensor).  
Each row in the dataset should represent a flattened sequence, and the last column should contain the target label/value.

```bash
python main.py --model lstm_attention --data_path ./data/your_dataset.csv --epochs 100
```

You can also experiment with:

```bash
python main.py --model rnn
```

---

## âš™ï¸ Configuration

Edit `config.py` to modify hyperparameters:

```python
class Config:
    input_size = 1
    hidden_size = 32
    output_size = 1
    num_layers = 1
    bidirectional = True
    batch_size = 16
    lr = 1e-3
```

---

## ðŸ“Š Metrics & Evaluation

During evaluation, the framework computes:

- **MSE** â€” Mean Squared Error
- **RMSE** â€” Root Mean Squared Error
- **RÂ² Score** â€” Coefficient of Determination

Metrics are logged directly in the terminal during evaluation.

---

## ðŸ§± Research Motivation

This repository is part of my ongoing **PyTorch Deep Research Grind** â€” a personal mastery track to understand AI model internals at a research-ready level.

Instead of relying on high-level libraries, this project breaks down every architectural component (recurrent units, gating mechanisms, attention scoring, etc.) to achieve *first-principles understanding* of how modern sequence models function.

> The objective wasn't to build a flashy demo â€” it was to learn how things actually work under the hood, the way DeepMind or FAIR research engineers would.

---

## ðŸ’¡ Key Learnings

- Built recurrent networks directly from matrix operations (`Wxh`, `Whh`, etc.)
- Understood hidden state propagation and gradient flow through time
- Integrated attention mechanisms for dynamic sequence weighting
- Implemented modular, reusable PyTorch components with clean documentation
- Structured research-grade training pipelines (config, utils, loaders, etc.)

---

## ðŸ§° Requirements

Install dependencies using:

```bash
pip install -r requirements.txt
```

**`requirements.txt`**

```
torch
numpy
pandas
scikit-learn
```

---

## ðŸ§¾ Citation / Usage

If you use this implementation as a reference for your own research or coursework, please cite:

```
Arnav Mishra, "RNNâ€“LSTMâ€“Attention: A Modular Sequence Modeling Implementation in PyTorch (Research Exercise Repository)", 2025.
```

---

## ðŸ§  Future Additions

- GRU and Transformer-based extensions
- Multi-feature sequence support
- Layer-wise introspection and visualization
- TorchScript / ONNX export utilities

---

## ðŸ Author

**Arnav Mishra**  
AI Researcher Â· PyTorch Core & Sequence Modeling Enthusiast  
Bhopal, India

---

> *"The only way to truly master PyTorch is to break it open and rebuild it yourself."*  
> â€” A guiding thought behind this repository.
